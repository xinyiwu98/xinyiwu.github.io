---
title: 'Oversmoothing in GNNs: why does it happen so fast? and do popular solutions (residual connections, normalization etc.) really work?'
date: 2023-06-05
permalink: /posts/oversmoothing-in-shallow-gnns/
tags:
  - GNNs
  - oversmoothing
---

Oversmoothing is a well-known problem in message-passing GNNs. But why does it happen when a GNN only have 2-4 layers? In this post, I am going to discuss our ICLR'23 paper "[A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks](https://arxiv.org/abs/2212.10701)", which develops the first finite depth theory of oversmoothing in graph neural networks (GNNs) and explains why oversmoothing would occur at a shallow depth.

Introduction
======



Most GNNs follow the message-passing paradigm, where  node representations are computed by recursively aggregating and transforming the representations of the neighboring nodes. One notably drawback of this repeated message-passing is **oversmoothing**, which refers to the phenomenon that increasing model depth leads to homogeneous node representations and thus worse downstream task performances. As a result, many GNNs used in practice remain shallow and often only have a few layers. 

Typical GNN performances with respect to the model depth would look like the following plot, where increasing network depth will first increase the performance, and once the model bypasses its optimal depth, which is typically a small number (e.g. 2-4 layers), the performance will quickly decrease.

![](images/performance_0.png)


Oversmoothing was something researchers noticed even back to the earliest days of the GNN field, when Kipf and Welling introduced the Graph Convolutional Network (GCN). So what has been understood about this performance plot in the past? 

- As the number of layers tends to infinity, oversmoothing is inevitable. 
- Having "some" graph convolutions is better than having no graph convolutions.
- Combining the above two points of views, the optimal depth is finite, i.e. curve should indeed look like a hump shape. 

It seems that we have understood everything about this plot. So what hasn't been understood about oversmoothing? Let me emphasize the following point: the occurrence of oversmoothing itself in the asymptotic regime is not sufficient to make it an important empirical problem. For example, if one trains a CNN(ResNet) for image classification with increasing depth, the performance would have a similar shape as the curve in the above plot, except now the peak is at, say 100 layers. In that case, oversmoothing is no longer a problem. **It is the fast onset of oversmoothing makes it an important problem in practice for developing more powerful GNNs and there has not been a statisfactory theory about that.**

As such, the main goal of our work is to answer the following two questions: 

- Why does oversmoothing happen at a relatively shallow depth?
- Can we quantitatively model the effect of applying a finite number of graph convolutions and
theoretically predict the “sweet spot” for the choice of depth?

Two counteracting effects (one desirable and one undesirable) of message-passing
======
Message-passing homogenizes node represetnations. However, we should distinguish between two kinds of homogenization:
- **Denoising:** nodes within the same class become similar. This will make the classification task easier.
- **Mixing:** nodes among different classes become similar. This will make the classification task more difficult.

In our analysis, we formulize the above two effects

Do popular proposed solutions to oversmoothing really work?
======

TL;DR: Not really. The problem of many solutions is that they are based on the understanding of oversmoothing in the asymptotic regime, which becomes incomplete in the non-asymptotic case as we have seen. Therefore, with the new insights we develop, these methods can actually be proven to be worse than vanilla GNNs on certain graphs. Specifically, in our paper, we extend the analysis to APPNP/GCNII, which are equivalent but has different motivations: APPNP uses the (approximate) Personalized PageRank propagation to replace the original graph convolution, and GCNII adds a residual connection to the initial node representations from each subsequent layer.
The 


   Similar reasoning and analysis can be also applied to other methods like PairNorm.








[def]: images/performance_0.pdf